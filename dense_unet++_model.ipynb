{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dense_unet++_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqkRNvEgEvFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolution Block \n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(conv_block, self).__init__()\n",
        "        \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        " \n",
        "class up_conv(nn.Module):\n",
        "    \"\"\"\n",
        "    Up Convolution Block\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(up_conv, self).__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "class conv_block_nested(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(conv_block_nested, self).__init__()\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
        "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.activation(x)\n",
        "        output = self.dropout(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "class DUpsampling(nn.Module):\n",
        "    def __init__(self, inplanes, scale, num_class=64, pad=0):\n",
        "        super(DUpsampling, self).__init__()\n",
        "        ## W matrix\n",
        "        self.conv_w = nn.Conv2d(inplanes, num_class * scale * scale, kernel_size=1, padding = pad,bias=False)\n",
        "        ## P matrix\n",
        "        self.conv_p = nn.Conv2d(num_class * scale * scale, inplanes, kernel_size=1, padding = pad,bias=False)\n",
        "\n",
        "        self.scale = scale\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv_w(x)\n",
        "        N, C, H, W = x.size()\n",
        "\n",
        "        # N, W, H, C\n",
        "        x_permuted = x.permute(0, 3, 2, 1) \n",
        "\n",
        "        # N, W, H*scale, C/scale\n",
        "        x_permuted = x_permuted.contiguous().view((N, W, H * self.scale, int(C / (self.scale))))\n",
        "\n",
        "        # N, H*scale, W, C/scale\n",
        "        x_permuted = x_permuted.permute(0, 2, 1, 3)\n",
        "        # N, H*scale, W*scale, C/(scale**2)\n",
        "        x_permuted = x_permuted.contiguous().view((N, W * self.scale, H * self.scale, int(C / (self.scale * self.scale))))\n",
        "\n",
        "        # N, C/(scale**2), H*scale, W*scale\n",
        "        x = x_permuted.permute(0, 3, 1, 2)\n",
        "        \n",
        "        return x\n",
        "\n",
        "### Modified NestedUnet_V2\n",
        "class Modified_NestedUNet_V2(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of this paper:\n",
        "    https://arxiv.org/pdf/1807.10165.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=3, out_ch=1):\n",
        "        super(Modified_NestedUNet_V2, self).__init__()\n",
        "        \n",
        "        ### This is a function from CELL 3 that we take the pre-trained weights.\n",
        "        self.encoder = get_encoder('resnet50', encoder_weights='imagenet')\n",
        "        \n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 4, n1 * 8, n1 * 16, n1 * 32]\n",
        "\n",
        "        # Encoder\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.Up1 = nn.Upsample(size=224, mode='bilinear', align_corners=True)\n",
        "        self.Up2 = nn.Upsample(size=56, mode='bilinear', align_corners=True)\n",
        "        self.Up3 = nn.Upsample(size=28, mode='bilinear', align_corners=True)\n",
        "        \n",
        "        self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0]) #(64+256, 64, 64)\n",
        "        self.conv0_2 = conv_block_nested(filters[0]*2 + filters[1] + filters[2], filters[0], filters[0]) #(64*2+256*2, 64, 64)\n",
        "        self.conv0_3 = conv_block_nested(filters[0]*3 + filters[1] + filters[2] + filters[3], filters[0], filters[0]) #(64*3+256, 64, 64)\n",
        "        \n",
        "        self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1])\n",
        "        self.conv1_2 = conv_block_nested(filters[1]*2 + filters[2] + filters[3], filters[1], filters[1])\n",
        "        self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])\n",
        "        \n",
        "        # Decoder\n",
        "        self.conv1 = nn.Conv2d(filters[4], filters[3], kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(filters[3])\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(filters[3], filters[2], kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(filters[2])\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(filters[2], filters[1], kernel_size=3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(filters[1])\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.conv4 = nn.Conv2d(filters[1], filters[0], kernel_size=3, padding=1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(filters[0])\n",
        "        self.dropout4 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.dupsample4 = DUpsampling(filters[0], 32, num_class=64)\n",
        "        \n",
        "        self.conv6 = nn.Conv2d(filters[0]*2, filters[0], kernel_size=3, padding=1, bias=False)\n",
        "        self.bn6 = nn.BatchNorm2d(filters[0])\n",
        "        self.dropout6 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.final = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # this part self.encoder(x) takes as input our input x and passes to the encoder of Res-Net which we implemented above in CELL 3.\n",
        "        # This gives as output 5 stacks of feature maps with different resoultions as we see below.\n",
        "        e = self.encoder(x)\n",
        "        #print(e[0].shape) #  1, 2048, 7, 7\n",
        "        #print(e[1].shape) #  1, 1024, 14, 14\n",
        "        #print(e[2].shape) #  1, 512, 28, 28\n",
        "        #print(e[3].shape) #  1, 256, 56, 56\n",
        "        #print(e[4].shape) #  1, 64, 112, 112 #after upsampling it is 64x224x224\n",
        "        \n",
        "        x0_0 = self.Up(e[4])\n",
        "        x1_0 = e[3]\n",
        "        x2_0 = e[2]\n",
        "        x3_0 = e[1]\n",
        "        x4_0 = e[0]\n",
        "        \n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self.Up1(x1_0)], 1))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up2(x2_1), self.Up2(x3_0)], 1))\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up1(x1_1), self.Up1(x2_0)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up1(x1_2), self.Up1(x3_0), self.Up1(x2_1)], 1)) \n",
        "        # x0_3 = 64 x 224 x 224\n",
        "        \n",
        "        # Decoder\n",
        "        x3_1 = self.conv1(x4_0)\n",
        "        x3_1 = self.bn1(x3_1)\n",
        "        x3_1 = self.relu(x3_1)\n",
        "        \n",
        "        x3_1 = self.conv2(x3_1)\n",
        "        x3_1 = self.bn2(x3_1)\n",
        "        x3_1 = self.relu(x3_1)\n",
        "        x3_1 = self.dropout2(x3_1)\n",
        "        \n",
        "        x3_1 = self.conv3(x3_1)\n",
        "        x3_1 = self.bn3(x3_1)\n",
        "        x3_1 = self.relu(x3_1)\n",
        "        x3_1 = self.dropout3(x3_1)\n",
        "        \n",
        "        x3_1 = self.conv4(x3_1)\n",
        "        x3_1 = self.bn4(x3_1)\n",
        "        x3_1 = self.relu(x3_1)\n",
        "        x3_1 = self.dropout4(x3_1)\n",
        "        \n",
        "        x3_1_up = self.dupsample4(x3_1)\n",
        "\n",
        "        x0_4_cat = torch.cat((x3_1_up, x0_3), dim=1)\n",
        "        \n",
        "        x3_1_final = self.conv6(x0_4_cat)\n",
        "        x3_1_final = self.bn6(x3_1_final)\n",
        "        x3_1_final = self.relu(x3_1_final)\n",
        "        x3_1_final = self.dropout6(x3_1_final)\n",
        "        \n",
        "        output = self.final(x3_1_final)\n",
        "       \n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}